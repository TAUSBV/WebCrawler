#broswer confirguration
browser = chromedriver
#browser = phantomjs
# timeout in seconds
browser.timeout = 60
browser.implicit.timeout = 5

#agent configuration
crawl.agent = Desktop agent: Mozilla/5.0 (X11; Linux x86_64; Storebot-Google/1.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36

# list of URLs to crawl (whitespace separated)
#crawl.starting.url.list = https://www.wsj.com/
# single URL if the list is not specified
crawl.starting.url = https://www.imf.org/en/News/Articles/2015/09/28/04/53/socar092513b

# crawling options
crawl.links = true
crawl.depth = 1
crawl.ignore.querystring = false
crawl.follow.ajax = true

# lucene options
crawl.index.folder = indexes

# total number of simultaneous threads
crawl.threads = 1

# frame to lookup inside the document
#element.frame = //iframe[@id='dsq-2']	
# value of element to look in xpath format
element.lookup = //article


# output settings
# output root folder
output.path = output
# output type text/html
ouput.type = text


# URL filters
#crawl.url.filter = robots|regex|extension
crawl.url.filter = regex|extension
crawl.url.filter.robots.class = net.taus.webcrawler.filter.RobotsFilter
crawl.url.filter.regex.class = net.taus.webcrawler.filter.RegexFilter
crawl.url.filter.regex.parameters = pattern
crawl.url.filter.regex.pattern = .*
#crawl.url.filter.regex.pattern = http.*((corona)|(covid)|(CORONA)|(COVID)).*
crawl.url.filter.extension.class = net.taus.webcrawler.filter.ExtensionFilter
crawl.url.filter.extension.parameters = ignore
crawl.url.filter.extension.ignore = jpg|jpeg|gif|png|js|css|svg|eot
