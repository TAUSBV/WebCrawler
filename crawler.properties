### Browser Configuration ###
# the value of the browser can be chromedriver or phantomjs
# timeout value is specified in seconds, it is the time to wait for any URL to fully load in the browser
# implicit timeout is for other javascript based events
browser.name = chromedriver
browser.base.location = drivers
browser.timeout = 60
browser.implicit.timeout = 5

# The agent string to use for crawling
crawl.agent = Mozilla/5.0 (Macintosh; Intel Mac OS X 11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36

### URL setting to crawl ###
# either a single URL or a list of URLs can be provided to crawl (whitespace separated)
# crawl.starting.url.list = https://www.example1.com \
# https://www.example2.com
#crawl.starting.url = https://www.example.com
crawl.starting.url.path = craw.list

### Crawling options ###
# set crawl links to true if you want to continue crawling all links found on the page
crawl.links = true
# depth will define how deep you want to go to crawl more links and pages
crawl.depth = 1
# if query string has no significance it can be ignored
crawl.ignore.querystring = false
# if javascript links to be followed or not
crawl.follow.ajax = true
# path where to create the lucene indexes to save the current state of the crawler
crawl.index.folder = indexes
# how many parallel crawling threads to start
crawl.threads = 1

#### Page specific configuration #############################################################

# If URL regex matches
element.lookup.list = pattern1|pattern2
element.lookup.pattern1.regex = http://example1.com/.* 
element.lookup.pattern1 = //section[@id='example1']

element.lookup.pattern2.regex = http://example2.com/.* 
element.lookup.pattern2 = //section[@id='example2']



# Default configuration
# if the content should be looked inside a particular iframe, the format of selector is xpaht
# element.frame = //iframe[@id='id1']
# which element to save where the main content of the page is available, if you don't know
# the element use //body
element.lookup = //section[@id='main']

# look for element with possible translated links
#element.languages = //ul[@class='lang']//a

##############################################################################################



###### Javascript Events #####################################################################

#browser.onload.triggers = trigger1
#browser.onload.trigger1.regex = http://example1.com/.*
#browser.onload.trigger1.js =

##############################################################################################


### URL filters ###
# which URL patterns to crawl and which to avoid
crawl.url.filter = robots|regex|extension 
crawl.url.filter.robots.class = net.taus.webcrawler.filter.RobotsFilter
crawl.url.filter.regex.class = net.taus.webcrawler.filter.RegexFilter
crawl.url.filter.regex.parameters = pattern
crawl.url.filter.regex.pattern = https://example.com/.*
crawl.url.filter.extension.class = net.taus.webcrawler.filter.ExtensionFilter
crawl.url.filter.extension.parameters = ignore
crawl.url.filter.extension.ignore = jpg|jpeg|gif|png|js|css|svg|eot